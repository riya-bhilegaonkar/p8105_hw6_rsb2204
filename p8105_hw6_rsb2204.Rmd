---
title: "Homework 6"
author: "Riya Bhilegaonkar"
date: "2022-11-27"
output: github_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1
To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


### Problem 2
Creating a `city_state` variable and a binary variable `resolved` to indicate whether a homicide is solved. Filtering for cities that do not include Dallas(TX), Phoenix(AZ), Kansas City(MO) and Tulsa(AL) and filtering for the victim race of white or black. The `victim_age` variable is changed to a numeric variable. 

```{r}
homicide_data = read_csv("data/homicide-data.csv") %>%
  janitor::clean_names() %>%
  mutate(city_state = str_c(city,state, sep=", "), resolved = as.numeric(disposition == "Closed by arrest"))%>%
  filter(city_state !="Dallas, TX" | city_state !="Phoenix, AZ" | city_state!="Kansas City, MO", victim_race == "White" | victim_race == "Black") %>%
  filter(city_state != "Tulsa, AL") %>%
  mutate(victim_age = as.numeric(victim_age), victim_race = fct_relevel(victim_race, "White"))%>%
  select(city_state, victim_sex, resolved, victim_age, victim_race)

homicide_data %>% 
  summarise(n = sum(is.na(victim_age)))
```

For the city of Baltimore, MD we use the `glm` function to fit a logistic regression with resolved vs unsolved as the outcome and victim age, sex and race as predictors, saving the results as an R object. 
```{r}
fit_logistic = homicide_data %>%
  filter(city_state == "Baltimore, MD")%>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data =., family = binomial()) 
```

Displaying the estimate and the confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
fit_logistic %>% 
  broom::tidy(conf.int=TRUE) %>% 
  mutate(OR = exp(estimate)) %>% 
   filter(term=="victim_sexMale")%>%
  mutate(CI.low = exp(conf.low), CI.high = exp(conf.high))%>%
  select(term, log_OR = estimate, OR, CI.low, CI.high) %>% 
  knitr::kable(digits = 3)
```

As this is a logistic regression we must remember to convert our OR back to an exponential from its log value and to do the same for the confidence interval values which we do using mutate().

Running a `glm` for each city in the dataset and extracting the adjusted odds ratio and CI for solving homicides comparing male victims to female victims. Creating a dataframe with estimated ORs and CIs for each city: 
```{r}
citiesOR_df = homicide_data %>%
  nest(data=-city_state)%>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_race + victim_sex + victim_age, data=.x, family = binomial()), na.action="na.exclude"),
    results = map(models, ~broom::tidy(x=.x, conf.int=T))) %>%
  select(-data, -models)%>%
  unnest(results)%>%
mutate(OR = exp(estimate)) %>%
    filter(term=="victim_sexMale")%>%
  mutate(CI.low = exp(conf.low), CI.high = exp(conf.high))%>%
  select(city_state, term, log_OR = estimate, OR, CI.low, CI.high)
  
```

Plot that shows the estimated ORs and CIs for each city:
```{r}
citiesOR_df %>%
mutate(city_state = fct_reorder(city_state, OR))%>%
ggplot(aes(x=city_state, y=OR))+
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
geom_errorbar(aes(ymin=CI.low, ymax=CI.high)) +
  labs(x = "city state")
```

From the plot we see that New York has the lowest odds ratio for solving homicides comparing male to female victims. The adjusted odds ratio is below 1 for the majority of cities which means the odds of solving a homicide in female victims is higher than in male victims. Albuquerque has the highest odds ratio where the estimated odds of a homicide being solved where there is male victim is  1.67 times the odds of a homicide being solved where there is a female victim. It is interesting to see these results as the majority of homicide victims are men.

### Problem 3

Loading and cleaning the data for regression analysis:
```{r}
birthweight = 
   read_csv("./data/birthweight.csv") %>%
  janitor::clean_names()%>%
  mutate(babysex = recode(
    babysex,
    "1" = "male",
    "2" = "female"
  ),
  frace = recode(
    frace,
    "1" = "White",
    "2" = "Black",
    "3" = "Asian",
    "4" = "Puerto Rican",
    "8" = "Other",
    "9" = "Unknown"
  ),
  malform = recode(
    malform,
    "0" = "absent",
    "1" = "present"
  ),
  mrace = recode(
    mrace,
    "1" = "White",
    "2" = "Black",
    "3" = "Asian",
    "4" = "Puerto Rican",
    "8" = "Other"
  )
  )
```

The birthweight dataset contains information that allows us to understand the effects of several variable such as a baby's sex, head circumference at birth, length at birth, birth weight, mother's weight at delivery, family monthly income etc on a child's birthweight. We first clean the data using `janitor::clean_names()`. For the purpose of our regression analysis, we convert the variables of `babysex`, frace`, `malform`, and `mrace` to factor variables. They are present in the intial dataset as categorical variables and their numeric coded values are not as intuitive to analysis as the recoded factor variables would be. 

Checking for missing values:
```{r}
birthweight%>%
  summarise(N_A = sum(is.na(birthweight)))
```

From the tibble we see that there appears to be no missing values in the data. 

We will use stepwise regression to determine an ideal regression model for birthweight:
```{r}
all_lm <- lm(bwt~., data=birthweight)

backward <- step(all_lm, direction='backward', scope=formula(all_lm), trace=0)

summary(backward)
```

Steps of Modelling process:
In order to chose the variables for our model, we use the stepwise approach which compares successive models and identifies the best subsets. For my model I used backward elimination. For this I first started with all the predictions in the model and then removed the predictors with the highest p-value (p-value) (those which are not significant <0.05). The `step()` with the direction of 'backward' function performs this analysis as it continues to re-fit the model and remove the next least significant predictor. The end results of the `step()`, the summary for the anova, shows which variables need to be removed from the model. 

```{r}
backward[["anova"]]

backward %>% anova
```

From these results we see that the variables of `wtgain`, `pnumsga`, `pnumlbw`, `frace`, `malform`, `ppbmi`, `momage`, and `menarche` can be removed from our model. 
Looking at the p-values we see that `babysex`, `bhead`, `blength`,  `delwt`, `fincome`, `gaweeks`,  `mheight`, `mrace`, `parity`, `ppwt` and `smoken` are significant at an alpha = 0.05 significance level and we will include these predictors in our model.

Using stepwise backward the chosen model is regression model is, this will be my proposed regression model:
```{r}
birthweight_lm <- lm(bwt~babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data=birthweight)

summary(birthweight_lm)
```

Here we can see summary statistics about the model including AIC and BIC values:
```{r}
birthweight_lm %>%
  broom::glance()%>%
  knitr::kable()
```

Plot of model residuals against fitted values:
```{r}
birthweight %>% 
modelr::add_residuals(birthweight_lm) %>%
modelr::add_predictions(birthweight_lm) %>% 
  ggplot(aes(x = pred, 
             y = resid)) +
  geom_point(alpha=0.4) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(
    x = "fitted values",
    y = "residuals",
    title = "Plot of model residuals against fitted values")
```

Comparing the proposed regression model against two others:

(a) One using length at birth and gestational age as predictors (main effects only)
```{r}
model1 = lm(bwt ~ blength + gaweeks, data=birthweight)
```

(b) One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
model2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*blength, data=birthweight)
```

Using cross-validated prediction errors to make the comparison:

```{r}
cv_df =
  modelr::crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    birthweight_lm  = map(train, ~lm(bwt~babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data=.x)),
    model1    = map(train, ~lm(bwt ~ blength + gaweeks, data=.x)),
    model2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*blength, data=.x))) %>% 
  mutate(
    rmse_birthweight = map2_dbl(birthweight_lm, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_model1    = map2_dbl(model1, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~modelr::rmse(model = .x, data = .y)))

```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From these results there is clearly an improvement in predictive accuracy gained through my proposed regression model in comparison to model 1 and model 2, as my model produces the lowest cross validated prediction error. In terms of ranking from lowest to highest prediction error it would first be my model, then model 2, then model 1. 
