---
title: "Homework 6"
author: "Riya Bhilegaonkar"
date: "2022-11-27"
output: github_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1


### Problem 2
Creating a `city_state` variable and a binary variable `resolved` to indicate whether a homicide is solved. Filtering for cities that do not include Dallas(TX), Phoenix(AZ), Kansas City(MO) and Tulsa(AL) and filtering for the victim race of white or black. The `victim_age` variable is changed to a numeric variable. 

```{r}
homicide_data = read_csv("data/homicide-data.csv") %>%
  janitor::clean_names() %>%
  mutate(city_state = str_c(city,    state, sep=", "), resolved = as.numeric(disposition == "Closed by arrest"))%>%
  filter(city_state !="Dallas, TX" | city_state !="Phoenix, AZ" | city_state!="Kansas City, MO" | city_state != "Tulsa, AL", victim_race == "White" | victim_race == "Black") %>%
  mutate(victim_age = as.numeric(victim_age), victim_race = fct_relevel(victim_race, "White"))%>%
  select(city_state, victim_sex, resolved, victim_age, victim_race)

```

For the city of Baltimore, MD we use the `glm` function to fit a logistic regression with resolved vs unsolved as the outcome and victim age, sex and race as predictors, saving the results as an R object. 
```{r}
fit_logistic = homicide_data %>%
  filter(city_state == "Baltimore, MD")%>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data =., family = binomial()) 
```

Displaying the estimate and the confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% # figure out CI calculations for OR??%>%
   filter(term=="victim_sexMale")%>%
  mutate(CI.low = OR - std.error*1.96, CI.high = OR + std.error*1.96)%>%
  select(term, log_OR = estimate, OR, CI.low, CI.high) %>% 
  knitr::kable(digits = 3)
```

Running a `glm` for each city in the dataset and extracting the adjusted odds ratio and CI for solving homicides comparing male victims to female victims. Creating a dataframe with estimated ORs and CIs for each city: 

```{r}
# fix this entirely
homicide_data %>%
  nest(data=-city_state)%>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_race + victim_sex + victim_age, data=homicide_data, family = binomial()), na.action="na.omit"),
    results = map(models, broom::tidy)) %>%
  select(-data, -models)%>%
  unnest(results)%>%
mutate(OR = exp(estimate)) %>% # figure out CI calculations for OR??
    filter(term=="victim_sexMale")%>%
  mutate(CI.low = OR - std.error*1.96, CI.high = OR + std.error*1.96)%>%
  select(city_state, term, log_OR = estimate, OR, CI.low, CI.high) %>% 
  knitr::kable(digits = 3)
  
```




### Problem 3

Loading and cleaning the data for regression analysis:
```{r}
birthweight = 
   read_csv("./data/birthweight.csv") %>%
  janitor::clean_names()%>%
  mutate(babysex = recode(
    babysex,
    "1" = "male",
    "2" = "female"
  ),
  frace = recode(
    frace,
    "1" = "White",
    "2" = "Black",
    "3" = "Asian",
    "4" = "Puerto Rican",
    "8" = "Other",
    "9" = "Unknown"
  ),
  malform = recode(
    malform,
    "0" = "absent",
    "1" = "present"
  ),
  mrace = recode(
    mrace,
    "1" = "White",
    "2" = "Black",
    "3" = "Asian",
    "4" = "Puerto Rican",
    "8" = "Other"
  )
  )
```

The birthweight dataset contains information that allows us to understand the effects of several variable such as a baby's sex, head circumference at birth, length at birth, birth weight, mother's weight at delivery, family monthly income etc on a child's birthweight. We first clean the data using `janitor::clean_names()`. For the purpose of our regression analysis, we convert the variables of `babysex`, frace`, `malform`, and `mrace` to factor variables. They are present in the intial dataset as categorical variables and their numeric coded values are not as intuitive to analysis as the recoded factor variables would be. 

Checking for missing values:
```{r}
birthweight%>%
  summarise(N_A = sum(is.na(birthweight)))
```

From the tibble we see that there appears to be no missing values in the data. 

We will use stepwise regression to determine an ideal regression model for birthweight:
```{r}
all_lm <- lm(bwt~., data=birthweight)

backward <- step(all_lm, direction='backward', scope=formula(all_lm), trace=0)

summary(backward)
```

Steps of Modelling process:
In order to chose the variables for our model, we use the stepwise approach which compares successive models and identifies the best subsets. For my model I used backward elimination. For this I first started with all the predictions in the model and then removed the predictors with the highest p-value (p-value) (those which are not significant <0.05). The `step()` with the direction of 'backward' function performs this analysis as it continues to re-fit the model and remove the next least significant predictor. The end results of the `step()`, the summary for the anova, shows which variables need to be removed from the model. 

```{r}
backward[["anova"]]

backward %>% anova
```

From these results we see that the variables of `wtgain`, `pnumsga`, `pnumlbw`, `frace`, `malform`, `ppbmi`, `momage`, and `menarche` can be removed from our model. 
Looking at the p-values we see that `babysex`, `bhead`, `blength`,  `delwt`, `fincome`, `gaweeks`,  `mheight`, `mrace`, `parity`, `ppwt` and `smoken` are significant at an alpha = 0.05 significance level and we will include these predictors in our model.

Using stepwise backward the chosen model is regression model is, this will be my proposed regression model:
```{r}
birthweight_lm <- lm(bwt~babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data=birthweight)

summary(birthweight_lm)
```

Here we can see summary statistics about the model including AIC and BIC values:
```{r}
birthweight_lm %>%
  broom::glance()%>%
  knitr::kable()
```

Plot of model residuals against fitted values:
```{r}
birthweight %>% 
modelr::add_residuals(birthweight_lm) %>%
modelr::add_predictions(birthweight_lm) %>% 
  ggplot(aes(x = pred, 
             y = resid)) +
  geom_point(alpha=0.4) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(
    x = "fitted values",
    y = "residuals",
    title = "Plot of model residuals against fitted values")
```

Comparing the proposed regression model against two others:

(a) One using length at birth and gestational age as predictors (main effects only)
```{r}
model1 = lm(bwt ~ blength + gaweeks, data=birthweight)
```

(b) One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
model2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*blength, data=birthweight)
```

Using cross-validated prediction errors to make the comparison:

```{r}
cv_df =
  modelr::crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    birthweight_lm  = map(train, ~lm(bwt~babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data=.x)),
    model1    = map(train, ~lm(bwt ~ blength + gaweeks, data=.x)),
    model2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*blength, data=.x))) %>% 
  mutate(
    rmse_birthweight = map2_dbl(birthweight_lm, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_model1    = map2_dbl(model1, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~modelr::rmse(model = .x, data = .y)))

```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From these results there is clearly an improvement in predictive accuracy gained through my proposed regression model in comparison to model 1 and model 2, as my model produces the lowest cross validated prediction error. In terms of ranking from lowest to highest prediction error it would first be my model, then model 2, then model 1. 
